---
title: "Report - meta analysis of the correlations"
author: "Living SEB app"
output:
  bookdown::html_document2
params:
  metaRes: NA
  descriptives: NA
  outcome: NA
  outlist: NA
  dm: NA
  bibfile: NA
bibliography: '`r normalizePath(params$bibfile, winslash = "/", mustWork = TRUE)`'
nocite: "@*"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(kableExtra)
library(ggplot2)

#   - Labels for the plots and tables
load_xlsx_url <- function(url) {
  temp_file <- tempfile(fileext = ".xlsx")
  tryCatch({
    download.file(url, destfile = temp_file, mode = "wb")
    readxl::read_excel(temp_file)
  }, error = function(e) {
    stop("Failed to download or read Excel file: ", e$message)
  })
}

admcol_url <- "https://raw.githubusercontent.com/feracotommaso/living_SEB_review/main/data/matrix_codebook.xlsx"
admcol <- load_xlsx_url(admcol_url)

varlist <- admcol[complete.cases(admcol[,1:4]),] # List of the variables that can be used with different grain [Topic, BROAD, specific, and label]
labels_app <- setNames(varlist$label, varlist$column_name) # Create a named vector from the lookup table

# Results and data from the app
descriptives <- params$descriptives
metaRes <- params$metaRes
metaRes$skill <- labels_app[as.character(metaRes$skill)]

out_selected <- params$outcome
out_selected <- labels_app[as.character(out_selected)]

outList <- params$outlist
dm <- params$dm

```

# Introduction

This report summarizes a three-level random-effects meta-analysis of the cross-sectional associations between the five SEB skill domains and the selected outcome (`r out_selected`), run via the Living SEB app. The app workflow consisted in the following steps:

-   Select the outcome and filters (e.g., publication years, age groups) and compile all eligible study effects.
-   Transform correlations from each study: Pearsonâ€™s ğ‘Ÿ values are converted to Fisherâ€™s ğ‘§ to stabilize variance.
-   Fit a three-level random-effects model (REML estimation) that separates sampling error from between-study and within-study/within-paper heterogeneity, so multiple effects from the same paper/sample are properly handled.
-   Pool effects and back-transform the modelâ€™s summary estimates from ğ‘§ back to ğ‘Ÿ for interpretation, with t-based confidence intervals.

In practical terms, the app computes a precision-weighted average of study-level Fisher's ğ‘§ correlations for each skillâ€“outcome pair, where larger samples carry more weight and a random-effects term allows true effects to vary across studies. Only studies that report a given pair contribute to that pairâ€™s estimate. For transparency, we report per-pair k (number of contributing studies) and Î£N (total participants contributing to that specific pair).

What â€œthree-levelâ€ means here. Effect sizes are nested (e.g., multiple correlations from the same sample or paper). A three-level model accounts for dependence by allowing variation within papers/samples and variation between papers, in addition to sampling error. This yields more accurate standard errors and heterogeneity estimates than treating all effects as independent.

General details of the analysis:

-   All materials, data, and code are available in the projectâ€™s GitHub repository.
-   Filtered data used for this run can be downloaded directly from the app.
-   Analyses were conducted in R (R Core Team, 2022) with metafor (Viechtbauer, 2010).
-   Pearsonâ€™s r correlation coefficients were transformed to Fisherâ€™s z scale for analysis. The results were then back-transformed to Pearsonâ€™s r
-   We employed three-level meta-analytic random-effects models using restricted maximum likelihood estimation. This approach accounts for heterogeneity both between and within studies.
-   Confidence intervals were estimated based on a t-distribution

## Descriptive statistics

The outcome selected for this meta-analysis is:

```{r}
print(out_selected)
```

A total of `r length(unique(dm$paper_id))` papers, `r length(unique(dm$matrix_id))` samples, and `r length(dm)` effects were considered for this meta-analysis.

The samples included in the meta-analysis are summarized in Table \@ref(tab:descriptives).

```{r descriptives}
knitr::kable(
  descriptives,
  caption = 'Samples included and corresponding sizes',
  align = c("lllccc")
)
```

# Results

The results of the meta-analysis are synthesized in the following table (Table \@ref(tab:metaRes)).

-   The strongest association appeared for `r round(metaRes$r[which.max(metaRes$r)],2)` with `r out_selected` (pooled *r* = `r metaRes$skill[which.max(metaRes$r)]`), while the weakest is `r metaRes$skill[which.min(metaRes$r)]` (pooled *r* = `r round(metaRes$r[which.min(metaRes$r)],2)`).
-   In total, `r sum(metaRes$p < 0.05)` pairs reached *p* \< 0.05 (`r sum(metaRes$p < 0.01)` at *p* \< 0.01; `r sum(metaRes$p < 0.001)` at *p* \< 0.001).
-   Use the forest plots to verify whether significant results reflect consistent evidence or are driven by a few influential studies.

```{r metaRes}
metaResTable <- data.frame(
      Skill = metaRes$skill,
      N = paste0(metaRes$N, " k = (", metaRes$k, ")"),
      r = format(round(metaRes$r, 3),nsmall=3),
      CI = paste0("[",format(round(metaRes$rcil,3),nsmall=3),"; ",
                  format(round(metaRes$rciu,3),nsmall=3),"]"),
      se = format(round(metaRes$se,3),nsmall=3),
      p = format(round(metaRes$p,4),3),
      q = paste0(round(metaRes$q), "(",metaRes$qdf,")",ifelse(metaRes$qp < .01, "*", "")),
      tau2 = format(round(metaRes$tau2,3),nsmall=3))

# metaResTable$Skill <- labels_app[as.character(metaResTable$Skill)]

knitr::kable(
  metaResTable,
  caption = 'Results of the meta-analysis',
  align = c("lccccccc")
)
```

Figure \@ref(fig:metaplot) shows the same results graphically. Confidence intervals are plotted together with point estimates.

```{r metaplot, fig.cap="Graphic representation of the results"}
metaRes$CI <- paste0("[",format(round(metaRes$rcil,3),nsmall=3),"; ",
                         format(round(metaRes$rciu,3),nsmall=3),"]")
      
ggplot(metaRes, aes(x = r, y = reorder(skill,5:1))) +
       geom_point(shape = 18,
                  color = "black",
                  size = 4) +
       geom_errorbar(aes(xmin = rcil, xmax = rciu), width = 0, linewidth = .8) +
       geom_vline(xintercept = c(.00), linetype = "dashed") +
       labs(x = "Meta-analytical association", y = "Skill") +
       theme_bw(base_size = 18) +
#       theme(strip.text = element_text(size = 18)) +
       annotate("text", x = metaRes$r,
                y = length(pred_vars):1+.20,
                label = paste0(round(metaRes$r,3)," ",metaRes$CI), 
                hjust = "center", size = 5) +
       coord_cartesian(xlim = c(ifelse(min(metaRes$rcil) > -.05, -.15,
                                       min(metaRes$rcil)),
                                 max(metaRes$rciu)))
```

## Forest plots

For each skillâ€“outcome pair, the forest plots display the study-level correlations (points with CIs) and the pooled estimate. This view is most useful for judging consistency across studies, spotting outliers, and understanding why a pair may have a wide CI or large ğœ2. Plotted values are shown on theğ‘Ÿscale (back-transformed from Fisherâ€™sğ‘§) for interpretability.

```{r}
for (i in 1:length(outList)) {
  this_var <- names(outList[i])
  this_var <- labels_app[as.character(this_var)]
  row_labels <- paste0(outList[[i]]$data$author_et_al, " ",
                       outList[[i]]$data$year)
  forest(outList[[i]],
         slab = row_labels,
         atransf = transf.ztor,
         pch = 19,
         xlab = "Fisher-transformed correlation (r)",
         main = paste("Outcome:", this_var)
  )
}
```

# References
